

import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import twitter_samples, stopwords
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
import pickle, re, string, random, tweepy, pandas,datetime, schedule
from os import environ



CONSUMER_KEY = 'CONSUMER_KEY'
CONSUMER_SECRET = 'CONSUMER_SECRET'
ACCESS_KEY = 'ACCESS_KEY'
ACCESS_SECRET = 'ACCESS_SECRET'

auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)
twitter_api = tweepy.API(auth, wait_on_rate_limit=True)


stop_words = set(stopwords.words('english')) 



def twitter_analytics(search_query,hash_search_query):
    
    tweets_number = 50
    positive_count = 0
    negative_count = 0
    tweet_messages = []
    tweet_message =[]

    today = datetime.date.today().strftime('%Y-%m-%d')
    date_object = datetime.date.today().strftime("%d-%b-%Y")
    time_object = datetime.datetime.now().strftime("%I:%M:%S %p")
    execution_start_time = datetime.datetime.now()

    tweets_dataframe = pandas.DataFrame()

    for tweet in tweepy.Cursor(twitter_api.search, q=search_query, since=today, lang = 'en', rpp=100).items(tweets_number):
    
    #Cleaning Tweets
        cleaned_tweet_text = re.sub(r'^.*?:', '',tweet.text)
        cleaned_tweet_text = re.sub(r'^https?:\/\/.*[\r\n]*', '', cleaned_tweet_text, flags=re.MULTILINE)
        cleaned_tweet_text = re.sub(r'Retweet',"",cleaned_tweet_text)
    
    
        tweet_message = [cleaned_tweet_text,tweet.source_url,tweet.user.screen_name, tweet.user.location,tweet.source,tweet.created_at]                    
        tweet_messages.append(tuple(tweet_message))


    tweets_dataframe = pandas.DataFrame(tweet_messages)
    tweets_dataframe.columns = ["Tweet_Text", "Tweet_URL", "Tweeted_By", "Tweeted_From", "Tweeted_Source","Tweeted_Time"]

    
    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]

    def remove_noise(tweet_tokens, stop_words = ()):

        cleaned_tokens = []

        for token, tag in pos_tag(tweet_tokens):
            token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|'                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)
            token = re.sub("(@[A-Za-z0-9_]+)","", token)

            if tag.startswith("NN"):
                pos = 'n'
            elif tag.startswith('VB'):
                pos = 'v'
            else:
                pos = 'a'

            lemmatizer = WordNetLemmatizer()
            token = lemmatizer.lemmatize(token, pos)

            if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:
                cleaned_tokens.append(token.lower())
        return cleaned_tokens

                                                                      
    loaded_model = pickle.load(open('sentiment_model.doc', 'rb'))


    for n in range(tweets_number):
    
        custom_tokens = remove_noise(word_tokenize(tweets_dataframe["Tweet_Text"][n]))
    
        filtered_sentence = [w for w in custom_tokens if not w in stop_words]
    
        filtered_sentence = [] 
  
        for w in custom_tokens: 
            if w not in stop_words: 
                filtered_sentence.append(w)
    
    
        result = loaded_model.classify(dict([token, True] for token in filtered_sentence))
    
        if result =='Positive' : 
            positive_count = positive_count + 1
        else :
            negative_count = negative_count + 1
            
    percentage_value = 100 * (positive_count/(positive_count+negative_count)) 

    main_message = "Realtime #Twitter trend about " +hash_search_query + " is " + str(round(percentage_value,2)) + "%" +" positive on "+date_object+" at "+time_object
    addon_message = "\n\nAnalysed tweets: " + str(tweets_number) 
    hash_tags_for_twitter = "\n\n#MachineLearning #TwitterBots #NaturalLanguageProcessing #NLP #Bots #DataScience #AI #ArtificialIntelligence"

    with open('temp.txt', 'w') as f:
       f.write("#BotsWarning\n\n" + main_message + addon_message + "\n\nNote: Generated by #TwitterBot" + hash_tags_for_twitter)

    with open('temp.txt','r') as f:
       twitter_api.update_status(f.read())



def main_pgm():

    personalities_list = ["Narendra Modi", "Xi Jinping", "Donald Trump"]

    for pers_one_by_one in range(len(personalities_list)):
    
        search_query = personalities_list[pers_one_by_one]
    
        hash_search_query = "#" + re.sub(r"\s+","", search_query)
        
        twitter_analytics(search_query,hash_search_query)
        


schedule.every(10).minutes.do(main_pgm)

while True: 
        schedule.run_pending()
    

